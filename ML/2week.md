# ML 2주차 정규과제

📌ML 정규과제는 매주 정해진 **유튜브 강의 영상을 통해 머신러닝 이론을 학습**한 후, 해당 내용을 바탕으로 **실습 문제를 풀어보며 이해도를 높이는 학습 방식**입니다. 

이번주는 아래의 **ML_2nd_TIL**에 명시된 유튜브 강의를 먼저 수강해 주세요. 학습 중에는 주요 개념을 스스로 정리하고, 이해가 어려운 부분은 강의 자료나 추가 자료를 참고해 보완해주세요. 과제까지 다 작성한 이후에 Github를 과제 시트에 제출해주시면 됩니다.



**(수행 인증샷은 필수입니다.)** 

> 주어진 과제를 다 한 이후, 인증샷이나 따로 코드를 깃허브에 정리하여 제출해주세요.



## ML_2nd_TIL

### 선형회귀모델 3 (파라미터 구간추정, 가설 검정)

### 선형회귀모델 4 (R², ANOVA)

<br>



## 주차별 학습 (Study Schedule)

| 주차   | 공부 범위                              | 완료 여부 |
| ------ | -------------------------------------- | --------- |
| 1주차. | 선형 회귀 (Linear Regression) (1)      | ✅         |
| 2주차  | 선형 회귀 (Linear Regression) (2)      | ✅         |
| 3주차  | 로지스틱 회귀 (Logistic Regression)    | 🍽️         |
| 4주차  | 결정 트리 (Decision Tree)              | 🍽️         |
| 5주차  | 앙상블 : 랜덤 포레스트 (Random Forest) | 🍽️         |
| 6주차  | 주성분 분석 (PCA)                      | 🍽️         |
| 7주차  | K - 평균 군집화                        | 🍽️         |

<!-- 여기까진 그대로 둬 주세요-->

---

# 1️⃣ 개념 정리

## 01. 선형회귀모델 3 (파라미터 구간추정, 가설검정)

```
✅ 학습 목표 :
* 점추정(Point Estimation)과 구간 추정(Interval Estimation)의 차이를 이해할 수 있다. 
* BLUE(Best Linear Unbiased Estimator)의 의미와 필요성을 이해할 수 있다.
* 기울기에 대한 가설검정의 원리와 p-value 해석 방법을 이해할 수 있다. 
```

### 선형 회귀 모델: 추정량의 추론과 가설검정

---

### 1. 추정(Estimation): 점추정과 구간 추정

선형 회귀 모델의 목표는 알 수 없는 파라미터($\beta_0, \beta_1$)를 추정하는 것 -> 추정에는 크게 두 가지 방법

* **점추정 (Point Estimation)**: 파라미터의 값을 하나의 특정 **점**으로 추정하는 방식
    * 최소제곱법(Least Squares Method)을 통해 구한 $\beta_0$와 $\beta_1$의 추정량($\beta_0, \beta_1$)이 바로 점추정량
    * 이 값들은 우리가 가진 데이터를 함수로 계산하여 나온 단일 값

* **구간 추정 (Interval Estimation)**: 파라미터의 값이 특정 **구간** 내에 존재할 것이라고 추정하는 방식
    * 점추정만으로는 추정값의 불확실성을 알기 어렵기 때문에, 신뢰구간을 통해 추정량에 대한 불확실성 정보를 제공
    * 구간 추정은 보통 '**점추정량 ± (상수값 × 표준편차)**'의 형태로 계산되며, 여기서 표준편차는 점추정량의 표준편차를 의미

---

### 2. BLUE(Best Linear Unbiased Estimator): 추정량의 좋은 성질

최소제곱법으로 구한 추정량($\beta_0, \beta_1$)은 통계적으로 매우 좋은 성질을 가지고 있으며, 이를 **BLUE**라고 함 -> BLUE는 다음 두 가지 핵심 속성을 만족

1.  **불편 추정량 (Unbiased Estimator)**: 추정량의 **기댓값**이 실제 파라미터의 값과 같음
    1. 즉, 표본에 따라 추정값은 변하더라도, 여러 번 반복했을 때 평균적으로는 참값과 일치
2.  **최소 분산 (Minimum Variance)**: 다른 어떤 불편 추정량과 비교했을 때 가장 **작은 분산**을 가짐
    1.  분산이 작다는 것은 추정량이 실제 참값 주변에 더 가깝게 모여 있다는 뜻으로, 더 정확한 추정량임을 의미

가우스-마르코프(Gauss-Markov) 정리에 따르면, 최소제곱법으로 구한 선형 회귀 모델의 추정량은 이 두 가지 조건을 모두 만족하는 최적의 추정량(BLUE)

---

### 3. 가설검정: 기울기의 유의성 판단

선형 회귀 모델에서 가장 중요한 가설검정 중 하나는 **기울기($\beta_1$)가 0인지**를 검정

* **가설 설정**:
    * **귀무가설 ($H_0$)**: $\beta_1 = 0$ (기울기가 0이므로, X 변수가 Y에 유의미한 영향을 미치지 않는다.)
    * **대립가설 ($H_1$)**: $\beta_1 \ne 0$ (기울기가 0이 아니므로, X 변수가 Y에 유의미한 영향을 미친다.)

* **검정통계량 (Test Statistic)**: 이 가설을 검정하기 위해 **'t-값'**이라는 검정통계량을 계산 -> 이 값은 추정량($\beta_1$)이 0과 얼마나 떨어져 있는지를 표준편차로 나눈 값
    * $t = (\beta_1 - 0) / ( \beta_1$의 표준편차$)$

* **P-value 해석**:
    * p-value는 귀무가설($H_0$)이 옳다는 가정하에, 현재 얻은 검정통계량($t$)과 같거나 더 극단적인 값이 나올 확률
    * **p-value가 매우 작으면 (일반적으로 0.05 미만)**: 우연히 이런 결과가 나올 확률이 거의 없다는 뜻이므로, 귀무가설($\beta_1 = 0$)이 틀렸다고 결론 내리고 기각 -> 이는 곧 **X 변수가 Y에 유의미한 영향을 미친다**고 해석
    * **p-value가 크면 (일반적으로 0.05 이상)**: 귀무가설을 기각할 충분한 증거가 없다는 뜻이므로, X 변수와 Y 사이에 유의미한 관계가 있다고 보기 어려움 



## 02.선형회귀모델 4 (R²) ANOVA)

```
✅ 학습 목표 :
* 결정계수(R²)와 수정 결정계수(Adjusted R²)의 의미와 차이를 이해할 수 있다.
* 결정계수를 통해 회귀모델의 설명력을 해석할 수 있다. 
* ANOVA(분산분석)의 개념과 회귀모델에서의 활용 방법을 이해할 수 있다. 
```

### 선형 회귀 모델: 결정계수와 분산분석

---

### 1. 결정계수($R^2$)와 수정 결정계수(Adjusted $R^2$)

결정계수($R^2$)는 회귀모델이 실제 데이터를 얼마나 잘 설명하는지 나타내는 지표

* **결정계수($R^2$)의 의미**: $R^2$는 **총 변동(SST)** 대비 **회귀모델이 설명하는 변동(SSR)**의 비율로 정의 -> 값은 0과 1 사이에 존재하며, 1에 가까울수록 모델의 설명력이 높다고 해석 (e.g. $R^2$가 0.683이라면, 사용된 X 변수들이 Y 변수의 변동성 중 약 68.3%를 설명한다고 볼 수 있음)
    * **$R^2 = SSR / SST = 1 - (SSE / SST)$**
    * **SST(Total Sum of Squares)**: 실제 Y 값과 Y의 평균값 간의 차이 제곱의 합 (Y의 총 변동성)
    * **SSR(Regression Sum of Squares)**: 회귀직선 위의 Y 값과 Y의 평균값 간의 차이 제곱의 합 (모델이 설명하는 변동성)
    * **SSE(Error Sum of Squares)**: 실제 Y 값과 회귀직선 위의 Y 값 간의 차이 제곱의 합 (모델이 설명하지 못하는 변동성)

* **수정 결정계수(Adjusted $R^2$)의 필요성**: $R^2$의 한계는 유의미하지 않은 변수를 추가해도 항상 증가하거나 유지된다는 점 -> 이는 모델의 복잡도만 높이고 설명력은 실질적으로 향상시키지 않는데도 $R^2$ 값이 부풀려지는 문제를 야기
    * **수정 결정계수**는 이 문제를 보완하기 위해 만들어짐 -> 모델에 추가되는 변수의 개수(p)와 샘플 수(n)를 고려하여 $R^2$를 보정
    * 유의미하지 않은 변수가 추가되면 오히려 수정 $R^2$가 감소하므로, 모델 간의 설명력을 더 공정하게 비교할 수 있음 

---

### 2. 분산분석(ANOVA)의 개념과 회귀모델에서의 활용

분산분석(Analysis of Variance, ANOVA)은 분산 정보를 활용하여 가설을 검정하는 통계적 기법 -> 선형 회귀모델에서는 **기울기($\beta_1$)가 0인지**를 판별하는 중요한 가설검정에 사용

* **ANOVA의 기본 개념**: ANOVA는 앞서 설명한 SST, SSR, SSE 세 가지 분산(제곱합)을 바탕으로 분석을 수행 -> 핵심은 **'회귀모델이 설명하는 분산(SSR)'**과 **'설명하지 못하는 분산(SSE)'**의 비율을 비교 
    * **SSR**은 X 변수에 의해 설명되는 정보, **SSE**는 오차(에러)에 의해 설명되는 정보

* **회귀모델에서의 활용**:
    1.  **가설 설정**:
        * **귀무가설 ($H_0$)**: $\beta_1 = 0$ (기울기는 0이다. 즉, X 변수가 Y에 유의미한 영향을 미치지 않는다.)
        * **대립가설 ($H_1$)**: $\beta_1 \neq 0$ (기울기는 0이 아니다. 즉, X 변수가 Y에 유의미한 영향을 미친다.)
    2.  **F-통계량 계산**: ANOVA는 **`SSR의 평균 제곱 / SSE의 평균 제곱`**으로 F-통계량을 계산 
        * 만약 F-통계량 값이 크다면(보통 1보다 훨씬 크다면), SSR이 SSE보다 훨씬 크다는 의미이므로, X 변수가 Y를 설명하는 힘이 강하다는 증거 
    3.  **결과 해석**: 계산된 F-통계량과 **p-value**를 통해 가설을 검정
        * **p-value가 0에 가까우면**: F-통계량 값이 매우 크다는 뜻이므로, 귀무가설($H_0$: $\beta_1 = 0$)을 기각 -> 이는 **X 변수가 Y에 통계적으로 유의미한 영향을 미친다**고 결론 내릴 수 있음 
        * 이러한 ANOVA 분석 결과는 회귀분석 결과표에서 F-통계량과 그에 대한 p-value로 제공 


<br>
<br>

---

# 2️⃣ 과제

> **1주차에 만든 모델에 대해 `statsmodels`라이브러리를 사용하여 회귀분석 리포트를 출력합시다. 각 변수의 p-value를 확인하고 유의미한 변수와 그렇지 않은 변수를 구분합니다. 모델의 R-squared와 Adj.R-squared 값을 해석하고, 이 모델이 데이터를 얼마나 잘 설명하는지 자신의 생각을 주피터 노트북에 작성하세요**



~~~
과제 가이드
1. statsmodels를 사용한 회귀모델 생성
- import statsmodels.api as sm 을 사용하세요
- X 변수에 상수항을 추가하세요 (sm.add_constatnt(X))
- OLS(최고 제곱법) 모델로 적합합니다.

2. 리포트 출력 및 해석 
- print(model.summary())
- 출력되는 summary 에서 아래 항목을 중심으로 해석하세요:
* 각 변수의 p-value
* R-squared / Adj. R-squared


(참고) 깃허브 Machine-Learning Template 레포지토리의 base_code 폴더에 week2 과제를 수행하기 위한 기본 베이스 코드가 제공되니, 이를 참고해도 되고, 자유롭게 진행하셔도 됩니다.  
~~~



<br>

### 🎉 수고하셨습니다.