# ML 7주차 정규과제

📌ML 정규과제는 매주 정해진 **유튜브 강의 영상을 통해 머신러닝 이론을 학습**한 후, 해당 내용을 바탕으로 **실습 문제를 풀어보며 이해도를 높이는 학습 방식**입니다. 

이번주는 아래의 **ML_7th_TIL**에 명시된 유튜브 강의를 먼저 수강해 주세요. 학습 중에는 주요 개념을 스스로 정리하고, 이해가 어려운 부분은 강의 자료나 추가 자료를 참고해 보완해주세요. 과제까지 다 작성한 이후에 Github를 과제 시트에 제출해주시면 됩니다.



**(수행 인증샷은 필수입니다.)** 

> 주어진 과제를 다 한 이후, 인증샷이나 따로 코드를 깃허브에 정리하여 제출해주세요.

**마지막 주차인만큼 끝까지 힘내주세요.!**

## ML_7th_TIL

### 군집분석

<br>



## 주차별 학습 (Study Schedule)

| 주차  | 공부 범위                              | 완료 여부 |
| ----- | -------------------------------------- | --------- |
| 1주차 | 선형 회귀 (Linear Regression) (1)      | ✅         |
| 2주차 | 선형 회귀 (Linear Regression) (2)      | ✅         |
| 3주차 | 로지스틱 회귀 (Logistic Regression)    | ✅         |
| 4주차 | 결정 트리 (Decision Tree)              | ✅         |
| 5주차 | 앙상블 : 랜덤 포레스트 (Random Forest) | ✅         |
| 6주차 | 주성분 분석 (PCA)                      | ✅         |
| 7주차 | K - 평균 군집화                        | ✅         |

<!-- 여기까진 그대로 둬 주세요-->



---

# 1️⃣ 개념 정리

## 01. 군집분석

```
✅ 학습 목표 :
* 군집화(Clustering)의 개념과 필요성을 이해할 수 있다.
* 군집화와 분류(Classification)의 차이를 이해할 수 있다.
* 유사도 측정 방법(거리 척도)과 군집화 알고리즘의 개념을 이해할 수 있다.
* 계층적 군집화와 K-means의 특징과 차이를 이해할 수 있다.
* 군집 수 결정 방법과 군집 평가 지표를 이해할 수 있다.
```

### 1. 군집화(Clustering)의 개념과 필요성

| 구분 | 내용 |
| :--- | :--- |
| **개념** | 주어진 관측치들(데이터) 간의 **유사성**을 측정하여, 비슷한 것끼리 묶어 그룹(군집)을 만드는 작업. |
| **목적** | 특정 값(**예측**이 목적이 아님)을 예측하는 대신, 데이터 내에 존재하는 **내재적 패턴**이나 **그룹 구조**를 발견하는 것. (비지도 학습) |
| **군집 기준** | 1. **동일 군집 내** 관측치 간 유사도 **최대화** (오밀조밀하게, Compact) |
| | 2. **서로 다른 군집 간** 유사도 **최소화** (되도록 멀리 떨어지게) |
| **적용 사례** | 고객 세분화(타겟 마케팅), 문서 군집(주제 분류), 센서 데이터 분석(오염 패턴 파악), 웨이퍼 불량 패턴 분석 등. |

---

### 2. 군집화(Clustering)와 분류(Classification)의 차이

| 구분 | 군집화 (Clustering) | 분류 (Classification) |
| :--- | :--- | :--- |
| **학습 유형** | **비지도 학습 (Unsupervised Learning)** | **지도 학습 (Supervised Learning)** |
| **핵심 변수** | **$Y$ (정답 레이블)이 없음.** | **$Y$ (정답 레이블)을 사용함.** |
| **목적** | 데이터 자체의 유사성을 기반으로 **그룹을 형성**하고 구조를 발견. | $X$를 이용해 $Y$의 범주를 **예측**하는 모델(결정 경계)을 생성. |

---

### 3. 유사도 측정 방법 (거리 척도) 및 군집화 알고리즘

#### A. 유사도 측정 (Similarity / Distance)
유사도는 주로 **거리(Distance)**로 표현되며, **거리가 멀수록 유사도가 떨어지고**, **거리가 가까울수록 유사도가 큽니다.**

* **유클리드 거리 (Euclidean Distance, L2 Norm)**: 두 점 사이의 **최단 직선 거리** (제곱 차이의 합에 제곱근 취함). 가장 흔히 사용됨.
* **맨하탄 거리 (Manhattan Distance, L1 Norm)**: 격자 형태를 가정하고 **좌표축을 따라 이동한 거리** (차이의 절댓값의 합).
* **마할라노비스 거리 (Mahalanobis Distance)**: 유클리드 거리와 유사하지만, 변수 간 **공분산**을 고려하여 측정함.
* **상관계수 거리 (Correlation Coefficient Distance)**: $1 - \text{상관계수 (R)}$로 계산하여, **패턴의 유사성**을 측정함. (값이 0이면 상관계수 1, 2이면 상관계수 -1)

#### B. 군집화 알고리즘의 분류
| 분류 | 특징 | 주요 방법 |
| :--- | :--- | :--- |
| **계층적 군집화** | 관측치를 하나씩 묶거나 나누면서 **계층적 구조**를 형성함. | 단일 연결, 완전 연결, 평균 연결, **워드(Ward)** 방법 등 |
| **분리형 군집화** | 전체 데이터 영역을 특정 기준에 의해 **동시에 분할**함. | **K-평균 군집화 (K-means)** |
| **기타** | 자기 조직화 지도(SOM), 분포 기반 군집화 등 | |

---

### 4. 계층적 군집화와 K-means의 특징과 차이

#### A. 계층적 군집화 (Hierarchical Clustering)
* **결과 표현**: **덴드로그램 (Dendrogram)** 또는 **클러스터링 트리**라는 시각적 계층 구조로 나타남.
* **특징**: 군집 레이블이 명확하게 부여되지 않아 **해석이 주관적**일 수 있음.
* **군집 간 거리**: 두 군집 간 거리를 어떻게 정의하느냐(최소 거리, 최대 거리, 평균 거리 등)에 따라 결과가 달라짐. 특히 **워드(Ward)** 방법은 병합 시 군집 내 편차(오차 제곱합) 증가를 최소화하는 방식으로 많이 사용됨.

#### B. K-평균 군집화 (K-means Clustering)
* **특징**:
    * **군집의 개수 (K)**를 **미리 지정**해야 함. (K-means의 K)
    * 초기 중심(Centroid)을 임의로 설정한 후, 반복(Iteration)을 통해 **군집 중심(평균)을 업데이트**하며 최적화함. (K-**평균**)
    * **군집 레이블이 명확히 부여**되므로 결과 해석이 객관적임.
* **단점**:
    * **초기 중심 설정**에 따라 최종 결과가 미세하게 달라질 수 있음. (반복 수행 등으로 해결)
    * **서로 다른 크기/밀도**의 군집이거나 **지역적 패턴(Local Pattern)**을 가진 데이터 형태(도넛, 바나나 모양 등)에 취약함.

---

### 5. 군집 수 결정 방법과 군집 평가 지표

| 구분 | 내용 |
| :--- | :--- |
| **최적 군집 수 (K) 결정** | 도메인 지식, 덴드로그램 시각화 또는 통계적 평가 지표를 통해 결정함. |
| **평가 지표 (클러스터링 성능)** | 1. **SSE (Sum of Squared Errors, 군집 내 오차 제곱합)**: 각 군집 중심으로부터 관측치까지의 거리 제곱의 합. **작을수록 좋음**. (군집 내 응집도만 고려함) |
| | 2. **실루엣 통계량 (Silhouette Score)**: 군집 내 응집도($a_i$)와 군집 간 분리도($b_i$)를 동시에 고려함. |
| | - **계산**: $\frac{b_i - a_i}{\max(a_i, b_i)}$ (분리도 $b_i$는 해당 관측치에서 다른 군집까지의 거리 중 **최솟값**, 응집도 $a_i$는 군집 내 다른 관측치까지의 **평균 거리**) |
| | - **범위**: **-1에서 1 사이**이며, **1에 가까울수록** 군집이 잘 되었음을 의미함. |
| **팁 (경험적)** | 실루엣 스코어로 평가 시, K=2일 때 가장 높게 나오는 경우가 많으므로, **두 번째로 높은 실루엣 스코어**를 보이는 K를 선택하는 것이 더 타당한 군집 구조일 수 있음. |



<br>
<br>

---

# 2️⃣ 과제

> **`Scikit-learn`에 내장된 붓꽃(Iris) 데이터셋을 사용하여 K-means 군집화를 실시합니다. 4개의 특성 중 2개(예: 꽃받침 길이, 꽃받침 너비)를 선택하여 2차원 평면에 시각화하고, K-Means 모델 (k = 3)을 적용했을 때 데이터가 어떻게 3개의 그룹으로 나뉘는지 확인합니다. 실제 품종(정답)과 모델이 분류한 군집을 나란히 시각화하여 비교해보는 주피터 노트북을 작성해보세요.**



~~~
과제 가이드
1. 데이터 로드
- from sklearn.datasets import load_iris 을 통해 불러오세요.

2. K-means 군집화 (k=3 수행)
- from sklearn.cluster import Kmeans 라이브러리를 사용하세요.
- 예시) k = 2 일때, 
kmeans = KMeans(n_cluster = 3, random_state = 0)


* 분석 포인트 & 힌트
- K-means의 레이블을 모르기에 클러스터 번호는 실제 품종과 일치하지 않을 수 있습니다. (비슷하게 그룹화했는지 여부로 확인하기)
- 계층적 군집화도 해보고, KMeans와도 비교해보기 
~~~



---

# 3️⃣ 실습 과제 (마지막 과제)

>  **🧚Q. 그동안의 배웠던 모델리을 적용해서 마지막 실습 과제를 진행해보려고 합니다. 가장 기본적으로 머신러닝의 기본이 되는 타이타닉 데이터셋을 활용하여 다양한 머신러닝 모델을 적용하고, 어떤 모델이 가장 높은 성능을 보이는지 비교, 분석을 진행합니다. 단순히 정확도를 비교하는 것이 아니라, 각 모델의 원리와 특성이 왜 해당 결과로 이어졌는지 논리적으로 해석하는 것이 핵심입니다.**
>
> (정규과제 업로드 시트에 과제를 수행한 Git 링크와 코랩도 같이 올려주세요. 코랩 주소는 2개를 올려주셔야 합니다.) 

~~~
과제 가이드라인

1. 데이터셋 불러오기 및 전처리
- 노션의 '너다나비' 에 있는 과제 가이드라인을 참고하세요. 

2. 모델별 학습 및 예측
- 다양한 모델을 동일한 데이터에 적용해 성능 차이를 비교합니다.
- 각 모델의 구조적 특성을 이해하고, 적합성의 이유를 설명합니다. 
- 예) 로지스틱 회귀, 결정 트리, 랜덤 포레스트 등 
(만약 강의에서 배우지 않은 다른 모델에서 성능이 더 높게 나오는 것이 있다면, 그 모델에 대한 추가 공부도 같이 하면 좋습니다.)

3. 성능 비교 및 시각화 진행

4. 추가 분석도 진행
- PCA를 통해 차원을 축소 한 이후에 로지스틱 회귀 및 랜덤 포레스트 비교
- K-Means를 통해 비지도적 군집화 -> 생존률이 유사한 그룹 시각화 
~~~



<!-- Machine Learning 트랙이 이렇게 마무리 되었습니다. 이번 트랙은 데이터 분석 과정에서 머신러닝을 단순히 활용하는데 그치지 않고, 각 모델이 어떤 원리로 동작하며 왜 특정한 상황에서 더 적합한지 이해하는 것에 초점을 두었습니다. 실제로 많은 데이터 분석 프로젝트에서는 단순히 여러 모델을 돌려가며 가장 성능이 좋은 것을 선택하는 것을 자주 볼 수 있습니다. 이보다는, 모델의 구조적 특성과 학습 원리를 이해하고 분석에 맞는 모델을 적용하는 것이 훨씬 더 중요합니다. 이번 과정을 통해 여러분이 앞으로 머신러닝 모델을 사용할 때 단순한 예측 도구가 아니라 깊은 통찰력으로 데이터를 해석하는 시각을 갖게 되길 바랍니다. 부족한 템플릿이지만 끝까지 따라와주셔서 진심으로 감사합니다.  -->

<br>

### 🎉 수고하셨습니다.