# ML 6주차 정규과제

📌ML 정규과제는 매주 정해진 **유튜브 강의 영상을 통해 머신러닝 이론을 학습**한 후, 해당 내용을 바탕으로 **실습 문제를 풀어보며 이해도를 높이는 학습 방식**입니다. 

이번주는 아래의 **ML_6th_TIL**에 명시된 유튜브 강의를 먼저 수강해 주세요. 학습 중에는 주요 개념을 스스로 정리하고, 이해가 어려운 부분은 강의 자료나 추가 자료를 참고해 보완해주세요. 과제까지 다 작성한 이후에 Github를 과제 시트에 제출해주시면 됩니다.



**(수행 인증샷은 필수입니다.)** 

> 주어진 과제를 다 한 이후, 인증샷이나 따로 코드를 깃허브에 정리하여 제출해주세요.



## ML_6th_TIL

### Principal Component Analysis(PCA, 주성분 분석)

<br>



## 주차별 학습 (Study Schedule)

| 주차  | 공부 범위                              | 완료 여부 |
| ----- | -------------------------------------- | --------- |
| 1주차 | 선형 회귀 (Linear Regression) (1)      | ✅         |
| 2주차 | 선형 회귀 (Linear Regression) (2)      | ✅         |
| 3주차 | 로지스틱 회귀 (Logistic Regression)    | ✅         |
| 4주차 | 결정 트리 (Decision Tree)              | ✅         |
| 5주차 | 앙상블 : 랜덤 포레스트 (Random Forest) | ✅         |
| 6주차 | 주성분 분석 (PCA)                      | ✅         |
| 7주차 | K - 평균 군집화                        | 🍽️         |

<!-- 여기까진 그대로 둬 주세요-->



---

# 1️⃣ 개념 정리

## 01. Principal Component Analysis(PCA, 주성분 분석)

```
✅ 학습 목표 :
* PCA(주성분 분석)의 필요성과 목적을 이해할 수 있다.
* 변수 선택과 변수 추출의 차이를 이해하고, PCA가 변수 추출 방식임을 이해할 수 있다.
* PCA의 수학적 원리(공분산 행렬, 고유값/고유벡터, 사영)를 이해할 수 있다.
* PCA 알고리즘의 절차를 이해하고 설명할 수 있다.
* PCA의 한계와 대안 기법을 이해할 수 있다. 
```

제시해주신 강의 내용을 바탕으로 PCA(주성분 분석)에 대한 핵심 내용을 요청하신 5가지 학습 목표에 따라 상세하고 명확하게 정리하였으며, 어미를 **'~함' 또는 '~임'** 형태로 통일하였습니다.

***


### 📌 PCA의 정의 및 개요
PCA(Principal Component Analysis)는 **고차원 데이터를 효과적으로 분석**하고 **차원을 축소**하여 데이터의 패턴을 파악하기 위한 비지도 학습(Unsupervised Learning) 기반의 분석 기법임.

### 주요 목적
1.  **차원 축소 (Dimensionality Reduction)**: $P$개의 변수로 구성된 원래 데이터를 **상관관계가 없는** $K$개의 새로운 변수($K < P$)로 요약함.
2.  **분산 최대화 (Variance Maximization)**: 원래 데이터가 갖고 있던 **정보량(분산)**을 최대한 보존하는 **새로운 축(주성분)**을 찾는 것이 핵심 목표임.
3.  **시각화 및 효율성 증대**: 고차원 데이터($P \ge 4$)는 시각화나 해석이 어렵고 모델링 효율이 떨어지므로, PCA를 통해 2차원 또는 3차원으로 줄여 **시각화, 해석, 모델링 효율**을 높임.

### 필요성 (고차원 데이터의 문제점)
이미지, 시그널, 네트워크, 텍스트와 같은 **고차원 데이터**는 다음과 같은 문제점을 가짐.
* 변수의 수가 많아 **불필요한 변수**가 존재할 가능성이 큼.
* 수학적 표현 및 해석이 어려움.
* 계산 복잡도가 증가하여 **모델링이 비효율적**일 가능성이 큼.

***

## 2. 변수 선택과 변수 추출의 차이를 이해하고, PCA가 변수 추출 방식임을 이해함

차원 축소 방법은 크게 두 가지로 나뉘며, PCA는 이 중 변수 추출(Feature Extraction)에 해당함.

| 구분 | 변수 선택 (Feature Selection) | 변수 추출 (Feature Extraction) |
| :--- | :--- | :--- |
| **개념** | 원래 변수($X_1, X_2, \dots$) 중 **중요한 변수만을 골라** 사용함. | 기존 변수들의 **선형 결합(Linear Combination)**을 통해 **새로운 변수($Z_1, Z_2, \dots$)를 정의**하여 사용함. |
| **PCA 해당 여부** | 해당 안 됨 | **해당됨** |
| **장점** | 원래 변수를 그대로 사용하므로 **해석이 용이**함. | 변수 간 **상관관계를 잘 고려**하며, 변수의 개수를 대폭 줄일 수 있음. |
| **단점** | 변수 간 상관관계를 고려하기 어려움. | 추출된 변수($Z$)가 원래 변수들의 결합이므로 **해석이 용이하지 않음**. |

주성분(Principal Component, $Z$)은 모든 원래 변수들($X$)의 선형 결합으로 생성됨.
$$Z = \alpha_1 X_1 + \alpha_2 X_2 + \dots + \alpha_p X_p$$
여기서 $\alpha$는 **로딩 벡터(Loading Vector)** 또는 **기저 벡터**라고 불림.

***

## 3. PCA의 수학적 원리(공분산 행렬, 고유값/고유벡터, 사영)를 이해함

PCA는 데이터를 새로운 축에 사영했을 때 **분산을 최대화**하는 축을 찾는 것을 수학적 기반으로 함.

### 1. 공분산 행렬 (Covariance Matrix, $\Sigma$)
* 다변량 데이터의 **상관관계에 대한 정보를 요약**하는 행렬임.
* 대각 원소에는 **각 변수의 분산**이, 비대각 원소에는 **두 변수 간의 공분산**이 담겨 있음.
* PCA는 이 공분산 행렬을 사용하여 데이터의 분산 구조를 파악하고, 이를 분해하여 새로운 축을 찾는 데 활용함.

### 2. 고유값과 고유벡터 (Eigenvalue & Eigenvector)
* **고유값 (Eigenvalue, $\lambda$)**: 행렬($\Sigma$)의 특성을 나타내는 값임. PCA에서 고유값 $\lambda_i$는 해당 **주성분($Z_i$)의 분산**을 나타냄.
* **고유벡터 (Eigenvector, $\alpha$)**: 행렬의 선형 변환에도 방향이 변하지 않는 특이한 벡터임. PCA에서 고유벡터 $\alpha_i$는 $i$번째 **주성분($Z_i$)의 축 방향**을 정의하며, 이것이 곧 주성분을 구성하는 **로딩 계수**가 됨.
    $$\Sigma \alpha = \lambda \alpha$$

### 3. 사영 (Projection)
* PCA의 핵심 철학은 **"새로운 축에 데이터를 사영했을 때, 그 사영된 데이터의 분산이 최대가 되는 축"**을 찾는 것임. (정보량 보존)

### 4. 최적화 문제
주성분을 찾는 과정은 다음과 같은 **최적화 문제**로 귀결됨.
$$\text{Maximize} \quad \text{Variance}(Z) = \text{Maximize} \quad \alpha^T \Sigma \alpha$$
$$\text{Subject to} \quad ||\alpha|| = 1 \quad (\text{로딩 벡터의 길이는 } 1)$$

이 문제의 해답은 결국 $\Sigma$ 행렬의 **가장 큰 고유값($\lambda_{max}$)에 해당하는 고유벡터($\alpha_{max}$)가 최적의 로딩 벡터($\alpha_1$)**가 된다는 것임.

***

## 4. PCA 알고리즘의 절차를 이해하고 설명함

PCA를 통해 주성분을 추출하고 차원을 축소하는 과정은 다음과 같음.

| 단계 | 설명 |
| :--- | :--- |
| **1. 데이터 정규화 (Min-Centering)** | 각 변수의 평균을 구해 원래 데이터에서 빼줌(평균이 0이 되도록). |
| **2. 공분산 행렬 계산** | 민 센터링된 데이터로부터 **공분산 행렬** 또는 **상관 행렬(Correlation Matrix)** $\Sigma$를 계산함. (일반적으로 상관 행렬 사용 권장) |
| **3. 고유값 및 고유벡터 계산** | $\Sigma$ 행렬의 고유값($\lambda$)과 이에 대응하는 고유벡터($\alpha$)를 계산함. |
| **4. 주성분 추출** | 고유값이 큰 순서대로 고유벡터를 정렬함. 가장 큰 고유값에 해당하는 고유벡터($\alpha_1$)가 **첫 번째 주성분($Z_1$)**의 로딩 벡터가 됨. |
| **5. 주성분 선택 (차원 축소)** | 추출된 주성분의 분산(고유값)을 분석하여 최종적으로 몇 개의 주성분을 선택할지 결정함. |

### 🔍 주성분 개수 선택 기준
1.  **누적 분산 비율 (Cumulative Variance Ratio)**: 주성분들의 **누적 분산 합**이 전체 분산의 **특정 비율(예: 70% ~ 90% 이상)**을 만족할 때까지의 주성분 개수를 선택함.
2.  **엘보 포인트 (Elbow Point)**: 고유값의 크기를 그래프로 그렸을 때, 고유값의 감소율이 급격히 낮아지는 **팔꿈치 모양의 지점(Elbow Point)**을 기준으로 주성분 개수를 선택함.

### 📝 주성분의 특징
* 추출된 주성분($Z_1, Z_2, \dots$)들은 서로 **공분산이 0**, 즉 **독립**임.

***

## 5. PCA의 한계와 대안 기법을 이해함

PCA는 강력한 기법이지만, 데이터의 분포 형태에 따라 한계점을 가짐.

### PCA의 한계점
1.  **가우시안 분포 가정의 한계**: PCA는 데이터 분포가 **단일 가우시안(정규 분포)**이거나 선형적인 관계일 때 가장 효과적임. 데이터 분포가 가우시안이 아니거나 **다중 모드(Multimodal)** 형태를 가질 경우, 데이터의 비선형적인 구조를 파악하지 못함.
2.  **분류/예측 성능 보장의 한계**: PCA는 **Y(종속 변수)를 전혀 이용하지 않는** 비지도 학습 방식임. 따라서 주성분은 **분산을 최대화**하는 방향으로만 추출되었을 뿐, 특정 분류 문제나 예측 문제의 최적 성능을 보장하는 방향으로 추출된 것은 아님.

### PCA의 대안 기법 (한계 극복을 위한 비선형 기법)
PCA가 비선형적인 구조를 파악하지 못하는 한계를 극복하기 위해 다음과 같은 **비선형 차원 축소 기법**들이 사용될 수 있음.

* **커널 PCA (Kernel PCA, KPCA)**: 커널 함수를 사용하여 데이터를 고차원 공간으로 변환하여 비선형적인 관계를 선형적으로 만든 후 PCA를 적용하는 방법임.
* **로컬 선형 임베딩 (Local Linear Embedding, LLE)**: 데이터의 국소적인(Local) 구조를 보존하면서 저차원으로 임베딩하는 방법임.
* **파셜 리스트 스퀘어 (Partial Least Squares, PLS)**: Y 변수를 고려하여 추출하는 **지도 학습 기반의 변수 추출** 방법임.



<br>
<br>

---

# 2️⃣ 과제

> **`Scikit-learn`의 손글씨 숫자(digits) 데이터셋을 로드합니다. 원본 데이터는 64개의 변수(8 X 8 픽셀)을 가집니다. PCA를 적용하여 데이터를 2개의 주성분으로 축소하고, 2차원 평면에 각 숫자가 어떻게 분포하는지 산점도로 시각화합니다. 첫 2개의 주성분이 전체 데이터 분산의 몇 %를 설명하는지 확인하고, 그 의미를 주피터 노트북에 서술하세요.**



~~~
과제 가이드
1. 데이터 로드
- from sklearn.datasets import load_digits 을 통해 불러오세요.

2. PCA 적용
- from sklearn.decomposition import PCA 라이브러리를 사용하시면 됩니다. 
- 예시) pca = PCA(n_components = k) 와 같이 사용하면 됩니다. 

3. 분석 설명
- print(f'PC1: {explained_var_ratio[0]:.2%}, PC2: {explained_var_ratio[1]:.2%}') 
- 첫 2개의 중성분이 전체 분산의 몇 %를 설명하는지 확인하기 

* 분석 포인트 
- 산점도를 해석하기 
- 분산 설명력을 해석하기 
	* PCA는 최대 분산 방향을 찾지만 분류 목적이 아닙니다. 
	* 고차원의 복잡한 구조를 단순화해서 보여주는 것입니다. 
~~~



<br>

### 🎉 수고하셨습니다.