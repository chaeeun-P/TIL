# ML 4주차 정규과제

📌ML 정규과제는 매주 정해진 **유튜브 강의 영상을 통해 머신러닝 이론을 학습**한 후, 해당 내용을 바탕으로 **실습 문제를 풀어보며 이해도를 높이는 학습 방식**입니다. 

이번주는 아래의 **ML_4th_TIL**에 명시된 유튜브 강의를 먼저 수강해 주세요. 학습 중에는 주요 개념을 스스로 정리하고, 이해가 어려운 부분은 강의 자료나 추가 자료를 참고해 보완해주세요. 과제까지 다 작성한 이후에 Github를 과제 시트에 제출해주시면 됩니다.



**(수행 인증샷은 필수입니다.)** 

> 주어진 과제를 다 한 이후, 인증샷이나 따로 코드를 깃허브에 정리하여 제출해주세요.



## ML_4th_TIL

### 의사결정나무모델 1 (모델 개요, 예측 나무)

### 의사결정나무모델 2 (분류 나무, Information Gain)

<br>



## 주차별 학습 (Study Schedule)

| 주차   | 공부 범위                              | 완료 여부 |
| ------ | -------------------------------------- | --------- |
| 1주차. | 선형 회귀 (Linear Regression) (1)      | ✅         |
| 2주차  | 선형 회귀 (Linear Regression) (2)      | ✅         |
| 3주차  | 로지스틱 회귀 (Logistic Regression)    | ✅         |
| 4주차  | 결정 트리 (Decision Tree)              | ✅         |
| 5주차  | 앙상블 : 랜덤 포레스트 (Random Forest) | 🍽️         |
| 6주차  | 주성분 분석 (PCA)                      | 🍽️         |
| 7주차  | K - 평균 군집화                        | 🍽️         |

<!-- 여기까진 그대로 둬 주세요-->



---

# 1️⃣ 개념 정리

## 01. 의사결정나무모델 1 (모델 개요, 예측 나무)

```
✅ 학습 목표 :
* 의사결정나무 (Decision Tree) 모델의 개념과 동작 방식을 이해할 수 있다.
* 예측나무 (Regression Tree)의 분할 방식과 예측 방법을 이해할 수 있다.
* 의사결정나무 모델의 데이터 분할 과정과 그 목적을 이해할 수 있다. 
```

요청하신 대로 의사결정나무 모델에 대한 강의 내용을 어미 **"~함"**으로 통일하여 정리함.

---

## 의사결정나무 (Decision Tree) 모델 개요 설명

의사결정나무 모델은 데이터에 내재된 패턴을 **변수의 조합**으로 파악하여 **예측 또는 분류**를 수행하는 모델을 나무 구조의 형태로 나타내는 것임.

### **1. 핵심 아이디어 및 특징**

* **질문을 통한 분할**: '스무고개'와 유사하게, 데이터를 **'예/아니오'**로 답할 수 있는 일련의 질문(조건)을 던져 전체 데이터 공간을 점점 좁혀나가는 방식으로 모델을 구성함.
* **나무 구조**: 복잡한 관계를 직관적인 **나무(Tree)** 형태로 표현하여 해석이 용이함.
* **모델 구축 단계**: 데이터(입력 $X$, 출력 $Y$)를 입력받아 알고리즘을 통해 모델을 구축함.

### **2. 나무 구조의 구성 요소**

의사결정나무는 뿌리 마디가 맨 위에 있는 **거꾸로 된 나무 형태**로 표현됨.

| 요소 | 설명 | 특징 |
| :--- | :--- | :--- |
| **뿌리 마디 (Root Node)** | 가장 위에 위치하며, 전체 데이터가 시작하는 유일한 마디임. | 전체 데이터 집합을 포함함. |
| **중간 마디 (Internal Node)** | 분할 기준이 적용되어 가지를 치는 마디임. | 아래에 또 다른 가지(자식 노드)가 존재함. |
| **끝 마디 (Leaf Node)** | 더 이상 분할(분지)이 일어나지 않는 최종 마디임. | 이 마디에 속한 데이터는 최종 예측값 또는 분류값을 가짐. |

* **이진 분할(Binary Splitting)**: 데이터를 기본적으로 두 개의 부분 집합으로 나누는 방식임. 모든 **끝 마디**에 속한 관측치 개수를 합하면 원래 데이터의 총 개수와 같아짐.

### **3. 데이터 분할의 목적 (균일성 확보)**

의사결정나무의 주요 목적은 데이터를 **균일(Homogeneous)해지도록 분할**하는 것임.

| 문제 유형 | $Y$의 형태 | 균일성의 정의 |
| :--- | :--- | :--- |
| **분류 문제** | 범주형 | 비슷한 **범주**를 가진 관측치끼리 모임. |
| **예측 문제** | 연속형 | 비슷한 **수치**를 가진 관측치끼리 모임. |

---

## **📉 예측나무 (Regression Tree) 모델 동작 방식**

예측나무는 출력 변수 $Y$가 **연속적인 숫자**일 때 적용되는 의사결정나무 모델임.

### **1. 예측나무의 예측 방법**

모델 구축 후 새로운 데이터가 입력되면, 해당 데이터가 속하게 되는 **끝 마디**의 정보를 사용하여 예측값을 결정함.

* **예측값 결정**: 데이터가 속한 부분 집합(끝 마디)에 포함된 **기존 관측치들의 $Y$ 값 평균**으로 예측함.
* **수식 표현**: 예측나무 모델 $f(x)$는 다음과 같은 상수들의 합으로 표현됨.

$$f(x) = \sum_{m=1}^{M} c_m I(x \in R_m)$$

* $R_m$: $m$번째 끝 마디(부분 집합) 영역임.
* $I(x \in R_m)$: 인디케이터 함수로, $x$가 $R_m$에 속하면 $1$, 아니면 $0$을 반환함.
* $c_m$: $R_m$ 지역의 예측값으로, **해당 지역 관측치 $Y$ 값의 평균**임.
* **핵심**: 비용 함수(Cost Function)인 잔차 제곱합(RSS)을 최소화하기 위한 $c_m$ 값은 해당 지역 $Y$ 값의 평균이 됨이 증명됨. 따라서 예측나무는 상수($Y$ 평균)로 예측하는 모델임.

### **2. 최적 분할 기준 결정 과정**

모델 구축 시, 매 단계마다 데이터를 가장 잘 분리하여 균일성을 높일 수 있는 **분할 변수($X_j$)**와 **분할점($s$)**을 결정함.

* **탐욕적 탐색 (Greedy Search)**: 모든 가능한 **분할 변수**와 해당 변수가 가질 수 있는 모든 **분할점** 후보들을 고려함.
* **최소화 기준**: 모든 경우를 비교하여 분할 후의 **비용 함수(RSS)**가 가장 **작아지는** 분할 기준($j, s$)을 선택함. 이 과정은 다음과 같은 RSS를 최소화하는 것을 목표로 함.

$$\min_{j, s} \left[ \sum_{x_i \in R_1(j, s)} (y_i - c_1)^2 + \sum_{x_i \in R_2(j, s)} (y_i - c_2)^2 \right]$$

* 이러한 방식으로 뿌리 마디에서 시작하여 각 중간 마디마다 최적의 분할이 이루어지며 예측나무 모델이 구축됨.



## 02. 의사결정나무모델 2 (분류 나무, Information Gain)

```
✅ 학습 목표 :
* 분류나무(Classification Tree) 모델의 구조와 동작 원리를 이해할 수 있다. 
* 불순도 지표(Gini, Cross-Entropy, Misclassification Rate)와 Information Gain의 개념을 이해할 수 있다.
* 분류나무 모델의 한계와 과적합 문제를 이해할 수 있다. 
```

제공해주신 두 번째 강의 내용을 바탕으로, **분류나무(Classification Tree)** 모델의 동작 원리, 불순도 개념, 그리고 모델의 한계점 및 과적합 문제를 학습 목표에 맞춰 "**~함**" 어미로 정리함.

---

## 분류나무 (Classification Tree) 모델 동작 원리 이해함

분류나무 모델은 출력 변수 $Y$가 **범주형(Class)**일 때 적용되는 의사결정나무 모델임. 예측나무와 마찬가지로 데이터를 **균일해지도록** 이진 분할함.

### **1. 분류나무의 구조 및 예측 방식 설명**

* **균일성 목적**: 분류 문제에서는 **비슷한 범주(클래스)**를 가진 관측치들끼리 모이도록 분할함.
* **예측(분류) 방법**: 새로운 관측치가 특정 **끝 마디**($R_m$)에 할당되면, 그 마디에 속한 기존 관측치들 중 **가장 많은 수**를 차지하는 범주(클래스)로 해당 관측치를 분류함 (**머저리티 클래스** 선택).

### **2. 분류나무의 수학적 표현 및 용어 정의**

* **끝 마디($R_m$)**: 분할을 멈춘 최종 부분 집합 영역을 의미.
* **관측치 개수($N_m$)**: 끝 마디 $R_m$에 속한 전체 관측치의 개수.
* **클래스 비율($p_{mk}$)**: 끝 마디 $R_m$ 내에서 특정 $k$ 클래스에 속한 관측치의 비율을 의미.
$$p_{mk} = \frac{\text{끝 마디 } R_m \text{ 내 } k \text{ 클래스 관측치 수}}{N_m}$$* **최종 분류 범주($K_m$)**: 끝 마디 $R_m$에 할당된 새로운 관측치의 분류 결과임. $p_{mk}$가 가장 큰 $k$ 클래스로 결정.$$K_m = \arg \max_{k} \{ p_{mk} \}$$

---

## **📉 불순도 지표와 Information Gain 개념**

분류나무를 구축할 때 최적의 분할 기준(변수와 분할점)을 찾기 위해 **불순도(Impurity)** 개념을 사용함. 불순도는 마디 내 데이터가 얼마나 혼재되어 있는지를 측정.

### **1. 불순도 지표 (비용 함수)**

불순도를 측정하는 대표적인 비용 함수 세 가지:

| 지표 | 공식 설명 | 특징 |
| :--- | :--- | :--- |
| **오분류율 (Misclassification Rate)** | $1 - \max_{k} \{ p_{mk} \}$ | 실제 범주와 모델의 분류 범주가 불일치하는 비율을 나타냄. 이것의 최소화를 목표로 함. |
| **지니 인덱스 (Gini Index)** | $\sum_{k=1}^{K} p_{mk} (1 - p_{mk})$ | 마디에서 무작위로 두 관측치를 뽑았을 때, 두 관측치가 서로 다른 클래스에 속할 확률을 나타냄. $0$에 가까울수록 순수함. |
| **크로스 엔트로피 (Cross-Entropy)** | $-\sum_{k=1}^{K} p_{mk} \log_2(p_{mk})$ | 불확실성(무질서도)을 나타내는 지표임. $0$에 가까울수록 순수함. |

### **2. Information Gain (정보 획득량) 설명**

Information Gain은 특정 변수를 사용하여 분할했을 때 **엔트로피(불순도)가 얼마나 크게 감소했는지**를 측정하는 지표.

$$\text{Gain}(S, A) = \text{Entropy}(S) - \sum_{v \in \text{Values}(A)} \frac{|S_v|}{|S|} \text{Entropy}(S_v)$$

* $\text{Entropy}(S)$: 분할 전 전체 데이터($S$)의 엔트로피(불순도).
* $\sum (\dots) \text{Entropy}(S_v)$: 변수 $A$로 분할한 후의 **가중 평균 엔트로피**.
* **분할 기준**: **Information Gain이 최대**가 되는 변수와 분할점을 선택. 이는 곧 엔트로피(무질서도)를 가장 많이 감소시키는 분할을 의미.

---

## **⚠️ 의사결정나무 모델의 한계 및 과적합 문제**

단일 의사결정나무 모델은 다음과 같은 단점을 가짐.

1.  **계층적 오류 전파**: 나무 구조의 상위 단계에서 발생한 오류가 수정 없이 하위 노드로 계속 전파되어 최종 결과에 영향을 미칠 수 있음.
2.  **불안정성**: 학습 데이터에 미세한 변동(노이즈)이 발생하더라도 최종 나무 구조와 예측 결과가 크게 변동될 수 있음.
3.  **과적합(Overfitting) 위험**:
    * 노드의 개수나 나무의 깊이(Depth)를 너무 많이 늘리면 **학습 데이터에 대한 오차(Bias)는 0에 가깝게** 만들 수 있음.
    * 하지만 이는 **분산(Variance)을 크게 키워** 새로운 데이터(테스트 데이터)에 대한 예측 성능(일반화 능력)이 떨어지는 **과적합** 문제를 초래함.

* **해결책**: 이러한 단점을 보완하기 위해 여러 개의 나무를 생성하고 그 결과를 요약하여 최종 예측을 수행하는 **앙상블(Ensemble)** 기법 (예: **랜덤 포레스트(Random Forest)**)이 등장하게 됨.



<br>
<br>

---

# 2️⃣ 과제

> **와인 품질 데이터셋을 사용하여 레드 / 화이트 와인을 분류하는 결정 트리 모델을 만들어봅시다. `plot_tree`함수로 트리 구조를 시각화하고, 와인 종류를 구분하는 가장 중요한 상위 2개의 분기 규칙이 무엇인지 주피터 노트북을 통해 작성해보세요.**



~~~
과제 가이드
1. 데이터 불러오기 및 준비
- 와인 품질 데이터셋 사용
- 레드 / 화이트를 이진 분류로 설정한다. (예시) red = 0, white = 1

2. 모델 학습
- from sklearn.tree import DecisionTreeClassifier, plot_tree 라이브러리 사용
- model.fit(X_train, y_train)으로 학습을 한다. 

* 힌트
- 트리의 루트노드로부터 상위 2~3개의 분기가 가장 핵심적인 변수입니다.
~~~



<br>

### 🎉 수고하셨습니다.